{"cells":[{"source":"# Building Multimodal AI Applications with LangChain & the OpenAI API ","metadata":{},"id":"bbf7941b-6c0a-4b14-b1fc-c703e57e352b","cell_type":"markdown"},{"source":"## Goals ","metadata":{},"id":"333b6c1a-a3c4-4c3c-b5e1-145bd214f4e0","cell_type":"markdown"},{"source":"Videos can be full of useful information, but getting hold of that info can be slow, since you need to watch the whole thing or try skipping through it. It can be much faster to use a bot to ask questions about the contents of the transcript.\n\nIn this project, you'll download a tutorial video from YouTube, transcribe the audio, and create a simple Q&A bot to ask questions about the content.","metadata":{},"cell_type":"markdown","id":"701b76fe-04db-405c-98f7-f0f5babd84b4"},{"source":"### Maintenance note, May 2024\n\nSince this code-along was released, the Python packages for working with the OpenAI API have changed their syntax. The instructions, hints, and code have been updated to use the latest syntax, but the video has not been updated. Consequently, it is now slightly out of sync. Trust the workbook, not the video.","metadata":{},"cell_type":"markdown","id":"7ae91e65-e94a-4858-b7b4-2316499f489d"},{"source":"- Understanding the building blocks of working with Multimodal AI projects\n- Working with some of the fundamental concepts of LangChain  \n- How to use the Whisper API to transcribe audio to text \n- How to combine both LangChain and Whisper API to create ask questions of any YouTube video ","metadata":{},"id":"3e302e1c-4c18-4c44-87fd-ba935c3a0853","cell_type":"markdown"},{"source":"## Before you begin","metadata":{},"id":"8231d2c6-275e-4399-b7cd-84e112831d08","cell_type":"markdown"},{"source":"You'll need a developer account with [OpenAI ](https://auth0.openai.com/u/signup/identifier?state=hKFo2SAyeTZBU1pzbUNWYWs3Wml5OWVvUVh4enZldC1LYU9PMaFur3VuaXZlcnNhbC1sb2dpbqN0aWTZIDFUakNoUGFMLUdNWFpfQkpqdncyZjVDQk9xUTE4U0xDo2NpZNkgRFJpdnNubTJNdTQyVDNLT3BxZHR3QjNOWXZpSFl6d0Q) and a create API Key. The API secret key will be stored in your 'Environment Variables' on the side menu. See the *getting-started.ipynb* notebook for details on setting this up.","metadata":{},"id":"785d7fac-edb2-482f-be2b-c63dc2882103","cell_type":"markdown"},{"source":"## Task 0: Setup","metadata":{},"id":"a9274661-8d8c-4cc5-901e-5fc497866b89","cell_type":"markdown"},{"source":"The project requires several packages that need to be installed into Workspace.\n\n- `langchain` is a framework for developing generative AI applications.\n- `yt_dlp` lets you download YouTube videos.\n- `tiktoken` converts text into tokens.\n- `docarray` makes it easier to work with multi-model data (in this case mixing audio and text).","metadata":{},"cell_type":"markdown","id":"823598ac-fa77-4532-997d-2923d0017e90"},{"source":"### Instructions\n\nRun the following code to install the packages.","metadata":{},"cell_type":"markdown","id":"c17ab340-c582-4ba7-ab33-5d582210f5c2"},{"source":"# Install the openai package, locked to version 1.27\n!pip install openai==1.27\n\n# Install the langchain package, locked to version 0.1.19\n!pip install langchain==0.1.19\n\n# Install the langchain-openai package, locked to version 0.1.6\n!pip install langchain-openai==0.1.6\n\n# Install the yt_dlp package, locked to version 2024.4.9\n!pip install yt_dlp==2024.4.9\n\n# Install the tiktoken package, locked to version 0.6.0\n!pip install tiktoken==0.6.0\n\n# Install the docarray package, locked to version 0.40.0\n!pip install docarray==0.40.0","metadata":{"executionCancelledAt":null,"executionTime":24111,"lastExecutedAt":1709640876005,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Lock OpenAI version to 0.27.1\n!pip install openai==0.27.1\n# Install langchain\n!pip install langchain==0.0.292","outputsMetadata":{"0":{"height":616,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"id":"e1ca41e3-2dfd-4b9a-b595-e5af720ca36a","cell_type":"code","execution_count":1,"outputs":[]},{"source":"### Instructions","metadata":{},"id":"92a9caca-70fd-4ac0-aa15-1bee55c456d3","cell_type":"markdown"},{"source":"## Task 1: Import The Required Libraries ","metadata":{},"cell_type":"markdown","id":"7f6c51c7-bbbb-4f0f-b218-850221f3dcdf"},{"source":"For this project we need the `os` and the `yt_dlp` packages to download the YouTube video of your choosing, convert it to an `.mp3` and save the file. We will also be using the `openai` package to make easy calls to the OpenAI models we will use. ","metadata":{},"cell_type":"markdown","id":"2cf847fd-f8f8-49f6-9b43-0eb098239072"},{"source":"### Instructions\n\nImport the following packages.\n\n- Import `os`. \n- Import `glob`.\n- Import `openai`.\n- Import `yt_dlp` with the alias `youtube_dl`.\n- From the `yt_dlp` package, import `DowloadError`.\n- Assign `openai_api_key` to `os.getenv(\"OPENAI_API_KEY\")`.","metadata":{},"id":"b1fcd794-b29c-4010-8be0-651a452b2044","cell_type":"markdown"},{"source":"# Import the os package\n\n\n# Import the glob package\n\n\n# Import the openai package \n\n\n# Import the yt_dlp package as youtube_dl\n\n\n# Import DownloadError from yt_dlp\n \n\n# Import DocArray \n\n","metadata":{"executionCancelledAt":null,"executionTime":176,"lastExecutedAt":1694705470957,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#Importing the Required Packages including: \"os\" \"openai\" \"yt_dlp as youtube_dl\" and \"from yt_dl import Download Error\"\n\nimport os   #import os package \nimport glob\nimport openai #import the openai package \nimport yt_dlp as youtube_dl #import the yt_dlp package as youtube_dl\nfrom yt_dlp import DownloadError #import DownloadError from yt_dlp ","outputsMetadata":{"0":{"height":77,"type":"stream"}}},"cell_type":"code","id":"541cd9f5-0aaa-4374-8411-25bedecd8c84","execution_count":3,"outputs":[]},{"source":"We will also assign the variable `openai_api_key` to the environment variable \"OPEN_AI_KEY\". This will help keep our key secure and remove the need to write it in the code here. ","metadata":{},"cell_type":"markdown","id":"794e2ce2-ba13-446f-9ac7-2b5743f65a51"},{"source":"openai_api_key = os.getenv(\"OPENAI_API_KEY\")","metadata":{"executionCancelledAt":null,"executionTime":10,"lastExecutedAt":1694705474406,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"openai_api_key = os.getenv(\"OPENAI_API_KEY\")"},"cell_type":"code","id":"7156b205-f844-4d9e-8867-449ff5840839","execution_count":4,"outputs":[]},{"source":"## Task 2: Download the YouTube Video","metadata":{},"cell_type":"markdown","id":"751b9539-cbf7-4d6e-9634-045345cf8a4a"},{"source":"After creating the setup, the first step we will need to do is download the video from Youtube and convert it to an audio file (.mp3). \n\nWe'll download a DataCamp tutorial about machine learning in Python.\n\nWe will do this by setting a variable to store the `youtube_url` and the `output_dir` that we want the file to be stored. \n\nThe `yt_dlp` allows us to download and convert in a few steps but does require a few configuration steps. This code is provided to you. \n\nLastly, we will create a loop that looks in the `output_dir` to find any .mp3 files. Then we will store those in a list called `audio_files` that will be used later to send each file to the Whisper model for transcription. ","metadata":{},"cell_type":"markdown","id":"48abc459-48e5-4c7c-a795-daaf347ceef6"},{"source":"### Instructions\n\n- Run the code to set the URL of the video, `youtube_url`, the directory to store the downloaded video, `youtube_url`, and the download settings, `ydl_config`.\n\n_This code can be adapted to any video you choose! Just change the URL._","metadata":{},"cell_type":"markdown","id":"8f2f2698-f768-4437-8e7f-c11327d3d4a7"},{"source":"# An example YouTube tutorial video\nyoutube_url = \"https://www.youtube.com/watch?v=aqzxYofJ_ck\"\n\n# Directory to store the downloaded video\noutput_dir = \"files/audio/\"\n\n# Config for youtube-dl\nydl_config = {\n    \"format\": \"bestaudio/best\",\n    \"postprocessors\": [\n        {\n            \"key\": \"FFmpegExtractAudio\",\n            \"preferredcodec\": \"mp3\",\n            \"preferredquality\": \"192\",\n        }\n    ],\n    \"outtmpl\": os.path.join(output_dir, \"%(title)s.%(ext)s\"),\n    \"verbose\": True\n}\n","metadata":{"executionCancelledAt":null,"executionTime":533,"lastExecutedAt":1694802393469,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# An example YouTube tutorial video\nyoutube_url = \"https://www.youtube.com/watch?v=aqzxYofJ_ck\"\n# Directory to store the downloaded video\noutput_dir = \"files/audio/\"\n\n# Config for youtube-dl\nydl_config = {\n    \"format\": \"bestaudio/best\",\n    \"postprocessors\": [\n        {\n            \"key\": \"FFmpegExtractAudio\",\n            \"preferredcodec\": \"mp3\",\n            \"preferredquality\": \"192\",\n        }\n    ],\n    \"outtmpl\": os.path.join(output_dir, \"%(title)s.%(ext)s\"),\n    \"verbose\": True\n}\n\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\nprint(f\"Downloading video from {youtube_url}\")\n\ntry:\n    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n        ydl.download([youtube_url])\nexcept DownloadError:\n    with youtube_dl.YoutubeDL(ydl_config) as ydl:\n        ydl.download([youtube_url])\n\n\n","outputsMetadata":{"0":{"height":357,"type":"stream"},"1":{"height":137,"type":"stream"},"2":{"height":97,"type":"stream"},"3":{"height":37,"type":"stream"},"4":{"height":257,"type":"stream"},"5":{"height":77,"type":"stream"},"6":{"height":57,"type":"stream"},"7":{"height":57,"type":"stream"},"8":{"height":97,"type":"stream"},"9":{"height":77,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"id":"ffb3836d-7b1b-47db-9ccc-6910972dd045","cell_type":"code","execution_count":6,"outputs":[]},{"source":"### Instructions\n\n- Check if `output_dir` exists, if not, then make that directory.\n- Try to download the video using the specified configuration.\n  -  If a DownloadError occurs, attempt to download the video again.","metadata":{},"cell_type":"markdown","id":"181a39f3-979b-411a-a53a-a7ab3a49d272"},{"source":"<details>\n<summary>Code hints</summary>\n<p>\n    \n- Check whether a directory exists using `os.path.exists()`.\n- Make a new directory with `os.makedirs()`.\n\n---\n    \nThe pattern for trying to do something then handling an error is:\n    \n```py\ntry:\n    # do something\nexcept TypeOfError:\n    # handle the error\n```\n\n---\n    \nThe YouTube downloader code pattern is similar to opening a file.\n\n```py\nwith youtube_dl.YoutubeDL(config_file) as ydl:\n    ydl.download([video_url])\n```\n\n</p>\n</details>","metadata":{},"cell_type":"markdown","id":"4fdc3d66-db63-46e2-95f9-c293be14d758"},{"source":"# Check if the output directory exists, if not create it\n\n\n# Print a message indicating which video is being downloaded\nprint(f\"Downloading video from {youtube_url}\")\n\n# Try to download the video using the specified configuration\n# If a DownloadError occurs, attempt to download the video again\n","metadata":{},"cell_type":"code","id":"69ae65cf-3109-490e-8a86-a9d98446ee67","outputs":[],"execution_count":null},{"source":"To find the audio files that we will use the `glob`module that looks in the `output_dir` to find any .mp3 files. Then we will append the file to a list called `audio_files`. This will be used later to send each file to the Whisper model for transcription. ","metadata":{},"cell_type":"markdown","id":"df9c586d-309a-411b-90a5-6e81fe85eda4"},{"source":"### Instructions\n\nFind the audio file in the output directory.\n\n- Find all the MP3 audio files in the output directory by joining the output directory to the pattern `*.mp3` and using glob to list them.\n- Select the first file in the list and assign it to `audio_filename`.\n- _Check your work._ Print `audio_filename`.","metadata":{},"cell_type":"markdown","id":"0fa69a42-7065-4c3f-8699-fe48908f11b1"},{"source":"# Find the audio file in the output directory\n\n# Find all the audio files in the output directory\n\n\n# Select the first audio file in the list\n\n\n# Print the name of the selected audio file\n","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1694705587367,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Find the audio file in the output directory\n\n# Define function parameters\noutput_dir = \"files/audio/\"\n\n# Find the audio file in the output directory\naudio_files = glob.glob(os.path.join(output_dir, \"*.mp3\"))\naudio_filename = audio_files[0]\nprint(audio_filename)","outputsMetadata":{"0":{"height":56,"type":"stream"}}},"cell_type":"code","id":"c3d0a34d-ade9-4314-bc7d-480f165b3992","execution_count":8,"outputs":[]},{"source":"## Task 3: Transcribe the Video using Whisper","metadata":{},"id":"a9fe2a4e-b6ac-43d3-9b22-7df437015913","cell_type":"markdown"},{"source":"In this step we will take the downloaded and converted Youtube video and send it to the Whisper model to be transcribed. To do this we will create variables for the `audio_file`, for the `output_file` and the model. \n\nUsing these variables we will:\n- create a list to store the transcripts\n- Read the Audio File \n- Send the file to the Whisper Model using the OpenAI package ","metadata":{},"cell_type":"markdown","id":"1a00b32c-06e2-4fb1-8830-3634b13d133a"},{"source":"### Instructions\n\nTranscribe the audio file.\n\n- _The audio file, output file, and model are definied for you._\n- Define an OpenAI client model. Assign to client.\n- Open the audio file as read-binary (`\"rb\"`).\n  - Use the Whisper model to create a transcription of the opened audio file. Assign to `response`.\n- Extract the transcript from the response.","metadata":{},"cell_type":"markdown","id":"e4b60c5a-ea58-469e-b699-d46ef1cc7485"},{"source":"<details>\n<summary>Code hints</summary>\n<p>\n \nDefine the client model with `openai.OpenAI()`.\n\n---\n\nThe code pattern for opening a file in read-binary mode is.\n    \n```py\nwith open(file, \"rb\") as file_handle:\n    # do something\n```\n\n---\n    \nTo use the OpenAI API to create transcription, call the client's `.audio.transcriptions.create()` method, passing the file and the model.\n\n```py\nresponse = client.audio.transcriptions.create(file=audio, model=model)\n```\n\n---\n    \nThe transcript text is in the `.text` element of the response.\n    \n</p>\n</details>","metadata":{},"cell_type":"markdown","id":"1b3137d7-4b28-4da3-aea3-852e6345579f"},{"source":"# Use these settings\naudio_file = audio_filename\noutput_file = \"files/transcripts/transcript.txt\"\nmodel = \"whisper-1\"\n\n# Transcribe the audio file to text using OpenAI API\nprint(\"converting audio to text...\")\n\n# Define an OpenAI client model. Assign to client.\n\n\n# Open the audio file as read-binary\n\n    # Use the model to create a transcription\n    \n\n# Extract the transcript from the response\n\n\n# Print the transcript\n","id":"54306dcc-40f7-4a12-97ef-388b95c70ad4","cell_type":"code","execution_count":9,"outputs":[],"metadata":{"executionCancelledAt":null,"executionTime":35820,"lastExecutedAt":1694705690478,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#Define function parameters\naudio_file = audio_filename\noutput_file = \"files/transcripts/transcript.txt\"\nmodel = \"whisper-1\"\n\n# Transcribe audio to text\nprint(audio_file)\nprint(\"converting audio to text...\")\nwith open(audio_file, \"rb\") as audio:\n    response = openai.Audio.transcribe(model, audio)\n\ntranscript = (response[\"text\"])\n\n","outputsMetadata":{"0":{"height":76,"type":"stream"}},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}}},{"source":"### Instructions\n\nSave the transcript to a text file.\n\n- If the directory for the output file doesn't exist, make it.\n- Write the transcript to the output file","metadata":{},"cell_type":"markdown","id":"0b7b4b2d-6e2b-4f86-b8b5-c58725c8de08"},{"source":"<details>\n<summary>Code hints</summary>\n<p>\n \n- Get the directory to write to with `os.path.dirname()` and the output file.\n- Make a directory with `os.makedirs()`. Set `exist_ok` to `True` to prevent errors if the directory already exists.\n\n---\n\nThe code pattern for writing a text file is as follows.\n    \n```py\nwith open(filename, \"w\") as file:\n    file.write(text)\n```\n\n\n</p>\n</details>","metadata":{},"cell_type":"markdown","id":"b30c40b2-ed8f-479c-a6ea-1b9f22359991"},{"source":"# Create the directory for the output file if it doesn't exist\n\n\n# Write the transcript to the output file\n\n\n","metadata":{"executionCancelledAt":null,"executionTime":27,"lastExecutedAt":1694705694076,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"if output_file is not None:\n    # save transcript to a .txt file\n    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n    with open(output_file, \"w\") as file:\n        file.write(transcript)\n\nprint(transcript)\n\n","outputsMetadata":{"0":{"height":532,"type":"stream"}},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"cell_type":"code","id":"6798bff4-ac8d-46e3-8c62-e540655e859d","execution_count":10,"outputs":[]},{"source":"## Task 4: Create a TextLoader using LangChain ","metadata":{},"cell_type":"markdown","id":"c1001454-eb29-4981-825f-fa08e2fc48e8"},{"source":"In order to use text or other types of data with LangChain we must first convert that data into Documents. This is done by using loaders. In this tutorial, we will use the `TextLoader` that will take the text from our transcript and load it into a document. ","metadata":{},"cell_type":"markdown","id":"8191715b-72ad-4a34-ac95-02e1fbf8d391"},{"source":"### Instructions\n\nLoad the documents from the text file using a TextLoader.\n\n- From the `langchain.document_loaders` module, import `TextLoader`.\n- Create a `TextLoader`, passing it the directory of the transcripts, `\"./files/text\"`. Assign to `loader`.\n- Use the TextLoader to load the documents. Assign to `docs`.","metadata":{},"cell_type":"markdown","id":"4f75f541-5bd7-4214-a75e-79681303c6f6"},{"source":"# From the langchain.document_loaders module, import TextLoader\n\n\n# Create a `TextLoader`, passing the directory of the transcripts. Assign to `loader`.\n\n\n# Use the TextLoader to load the documents. Assign to docs.\n","id":"bb8654f7-965e-4e62-98ab-d08b7026e3d9","cell_type":"code","outputs":[],"metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1694705724878,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.document_loaders import TextLoader\n\nloader = TextLoader(\"./files/text\")\ndocs = loader.load()"},"execution_count":11},{"source":"# Show the first element of docs to verify it has been loaded \n","metadata":{"executionCancelledAt":null,"executionTime":16,"lastExecutedAt":1694705727440,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"docs[0]","collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"id":"269aaed5-7d07-43d7-a2d0-a89730ec4bc9","cell_type":"code","execution_count":12,"outputs":[]},{"source":"## Task 5: Create an In-Memory Vector Store ","metadata":{},"cell_type":"markdown","id":"577069b3-02f6-4b73-aaaa-d8b8e8006d98"},{"source":"Now that we have created Documents of the transcription, we will store that Document in a vector store. Vector stores allows LLMs to traverse through data to find similiarity between different data based on their distance in space. \n\nFor large amounts of data, it is best to use a designated Vector Database. Since we are only using one transcript for this tutorial, we can create an in-memory vector store using the `docarray` package. \n\nWe will also tokenize our queries using the `tiktoken` package. This means that our query will be seperated into smaller parts either by phrases, words or characters. Each of these parts are assigned a token which helps the model \"understand\" the text and relationships with other tokens. ","metadata":{},"cell_type":"markdown","id":"79af9e43-c32f-478d-b057-dc3b7890925e"},{"source":"### Instructions\n\n- Import the `tiktoken` package. ","metadata":{},"cell_type":"markdown","id":"d3a5eb22-3a34-40a5-9f00-bd4895a1c4ca"},{"source":"# Import the tiktoken package\n","metadata":{"executionCancelledAt":null,"executionTime":46,"lastExecutedAt":1694705815702,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import tiktoken"},"id":"15298bd3-5465-450d-b917-5e5d87d78bf2","cell_type":"code","execution_count":14,"outputs":[]},{"source":"## Task 6: Create the Document Search ","metadata":{},"cell_type":"markdown","id":"6e01af9c-f5ea-4382-b7ff-01fe0bb30edc"},{"source":"We will now use LangChain to complete some important operations to create the Question and Answer experience. Let´s import the follwing: \n\n- Import `RetrievalQA` from `langchain.chains` - this chain first retrieves documents from an assigned Retriver and then runs a QA chain for answering over those documents \n- Import `ChatOpenAI` from `langchain.chat_models` - this imports the ChatOpenAI model that we will use to query the data \n- Import `DocArrayInMemorySearch` from `langchain.vectorstores` - this gives the ability to search over the vector store we have created. \n- Import `OpenAIEmbeddings` from `langchain.embeddings` - this will create embeddings for the data store in the vector store. \n- Import `display` and `Markdown`from `IPython.display` - this will create formatted responses to the queries. (","metadata":{},"cell_type":"markdown","id":"22438b44-b8f8-4c78-a573-87ee4bdb2234"},{"source":"# Import the RetrievalQA class from the langchain.chains module\n\n\n# Import the ChatOpenAI class from the langchain.chat_models module\n\n\n# Import the DocArrayInMemorySearch class from the langchain.vectorstores module\n\n\n# Import the OpenAIEmbeddings class from the langchain.embeddings module\n","metadata":{"executionCancelledAt":null,"executionTime":8,"lastExecutedAt":1694706214485,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from langchain.chains import RetrievalQA\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.vectorstores import DocArrayInMemorySearch\nfrom langchain.embeddings import OpenAIEmbeddings"},"id":"3a7fb40d-de20-4ec8-b05a-036b6dc6ad66","cell_type":"code","execution_count":20,"outputs":[]},{"source":"Now we will create a vector store that will use the `DocArrayInMemory` search methods which will search through the created embeddings created by the OpenAI Embeddings function. ","metadata":{},"cell_type":"markdown","id":"9bec39d2-8a4c-4638-953f-fbd9fa47ad6f"},{"source":"### Instructions\n\nCreate an in-memory search object from the specified documents, `docs` and embeddings, `OpenAIEmbeddings()`. Assign to `db`.","metadata":{},"cell_type":"markdown","id":"665d55d7-25fb-4aeb-9434-6b76de0ee405"},{"source":"<details>\n<summary>Code hints</summary>\n<p>\n \n- `DocArrayInMemorySearch` has a `.from_documents()` method that takes two arguments: the documents and the embeddings.\n- The documents were created when you used the `TextLoader`.\n- `OpenAIEmbeddings()` specifies the type of embeddings to use.\n\n</p>\n</details>","metadata":{},"cell_type":"markdown","id":"5385cba2-7d13-4368-a9b3-8b76d57ffe6c"},{"source":"# Create a new DocArrayInMemorySearch instance from the specified documents and embeddings\n","metadata":{"executionCancelledAt":null,"executionTime":502,"lastExecutedAt":1694706217261,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"db = DocArrayInMemorySearch.from_documents(\n    docs, \n    OpenAIEmbeddings()\n)"},"id":"66ef212c-eefd-4cf2-a02c-3c01b1b29118","cell_type":"code","execution_count":21,"outputs":[]},{"source":"We will now create a retriever from the `db` we created in the last step. This enables the retrieval of the stored embeddings. Since we are also using the `ChatOpenAI` model, will assigned that as our LLM.\n\nRecall that the temperature of an LLM refers to how random the results are. Setting the temperature to zero makes the results more repeatable.","metadata":{},"cell_type":"markdown","id":"033f3ebc-c098-49e8-a96f-f428940996d9"},{"source":"### Instructions\n\n- Convert the `DocArrayInMemorySearch` instance to a retriever. Assign to `retriever`.\n- Create a new `ChatOpenAI` instance with a temperature of `0.0`. Assign to `llm`. ","metadata":{},"cell_type":"markdown","id":"0aabff95-8fa2-47ea-b0b3-23c53c6d3c38"},{"source":"<details>\n<summary>Code hints</summary>\n<p>\n \n- `DocArrayInMemorySearch` has a `.as_retriever()` method to convert it to a retriever object. No arguments are required.\n- Create a client model with `ChatOpenAI()`, setting the `temperature` argument.\n\n</p>\n</details>","metadata":{},"cell_type":"markdown","id":"f005ea57-b2fa-4d13-9957-4d23d9b01d15"},{"source":"# Convert the DocArrayInMemorySearch instance to a retriever\n\n\n# Create a new ChatOpenAI instance with a temperature of 0.0\n","metadata":{"executionCancelledAt":null,"executionTime":8,"lastExecutedAt":1694706219264,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"retriever = db.as_retriever() \nllm = ChatOpenAI(temperature = 0.0)"},"id":"7c7f6113-c145-47ff-ab9c-ada04ca047ce","cell_type":"code","execution_count":22,"outputs":[]},{"source":"Our last step before starting to ask questions is to create the `RetrievalQA` chain. This chain takes in the:  \n- The `llm` we want to use.\n- The `chain_type` which is how the model retrieves the data. Here we will use a _stuff_ chain, where all the documents are stuffed into the prompt. It is the simplest type, but only works where you only have a few small documents.\n- The `retriever` that we have created.\n- An option called `verbose` that prints details of each step of the chain.","metadata":{},"cell_type":"markdown","id":"4f5a7c7a-1676-41e0-976a-50316a684d12"},{"source":"### Instructions\n\nCreate a new RetrievalQA instance from the chain type. \n\n- Set `llm` to the ChatOpenAI instance you just created.\n- Set `chain_type` to `\"stuff\"`.\n- Set `retriever` to the retriever you just created.\n- Set `verbose` to `True`.","metadata":{},"cell_type":"markdown","id":"2a5ce4f9-e025-40e6-b737-be77213d5110"},{"source":"<details>\n<summary>Code hints</summary>\n<p>\n \nThe code pattern to create a `RetrievalQA` instance from a chain type is as follows.\n    \n```py\nRetrievalQA.from_chain_type(\n    llm=your_model,\n    chain_type=your_chain_type,\n    retriever=your_retriever,\n    any_other_options\n)\n```\n\n</p>\n</details>","metadata":{},"cell_type":"markdown","id":"448bee81-92ea-4632-b250-d8f8ae90c64c"},{"source":"# Create a new RetrievalQA instance with the specified parameters\n\n    # The ChatOpenAI instance to use for generating responses\n    # The type of chain to use for the QA system\n    # The retriever to use for retrieving relevant documents\n    # Whether to print verbose output during retrieval and generation\n","metadata":{"executionCancelledAt":null,"executionTime":9,"lastExecutedAt":1694706178555,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"qa_stuff = RetrievalQA.from_chain_type(\n\nllm=llm,\n\nchain_type=\"stuff\",\n\nretriever=retriever,\n\nverbose=True\n\n)"},"id":"09fc202b-198f-4510-8d81-258f914d5c08","cell_type":"code","execution_count":18,"outputs":[]},{"source":"## Task 7: Create the Queries ","metadata":{},"cell_type":"markdown","id":"4ce8cff5-ab49-44f0-96ee-88826d88ea6a"},{"source":"Now we are ready to create queries about the YouTube video and read the responses from the LLM. This done first by creating a query and then running the RetrievalQA we setup in the last step and passing it the query. ","metadata":{},"cell_type":"markdown","id":"d51218d4-4e81-4d87-9f3f-77eacde057c1"},{"source":"### Instructions\n\nAsk GPT some questions about the transcript.\n\n- Create question, `\"What is this tutorial about?\"`. Assign to `query`.\n- Invoke the query through the RetrievalQA instance. Assign to `response`. \n- Print the response.","metadata":{},"cell_type":"markdown","id":"5e2b036b-cef6-4b52-9421-5ccf2b865482"},{"source":"<details>\n<summary>Code hints</summary>\n<p>\n \nCall the `.invoke()` method of the RetrievalQA instance, passing the query. The code pattern is as follows.\n    \n```py\nresponse = qa.invoke(text)\n```\n\n</p>\n</details>","metadata":{},"cell_type":"markdown","id":"5dc914bb-dba3-4880-90bc-15cd037ce911"},{"source":"# Set the query to be used for the QA system\n\n\n# Invoke the query through the RetrievalQA instance. Assign to response.\n\n\n# Print the response to the console\n\n","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":115,"type":"stream"}}},"id":"d576672c-5078-487a-9dc5-3703f17d82f1","cell_type":"code","execution_count":19,"outputs":[]},{"source":"We can continue on creating queries and even creating queries that we know would not be answered in this video to see how the model responds. ","metadata":{},"cell_type":"markdown","id":"de9f2df3-87d1-40d3-862a-95769c11d015"},{"source":"# Set the query to be used for the QA system\nquery = \"What is the difference between a training set and test set?\"\n\n# Invoke the query through the RetrievalQA instance and store the response\n\n\n# Print the response to the console\n","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":115,"type":"stream"}}},"id":"dbb75225-76c1-4eb8-9055-e13a3bb68682","cell_type":"code","execution_count":23,"outputs":[]},{"source":"# Set the query to be used for the QA system\nquery = \"Who should watch this lesson?\"\n\n# Invoke the query through the RetrievalQA instance and store the response\n\n\n# Print the response to the console\n","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":115,"type":"stream"}}},"cell_type":"code","id":"13864a14-0eda-4afd-bfe5-90fdefbc5d49","execution_count":24,"outputs":[]},{"source":"# Set the query to be used for the QA system\nquery = \"Who is the greatest football team on earth?\"\n\n# Invoke the query through the RetrievalQA instance and store the response\n\n\n# Print the response to the console\n","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":115,"type":"stream"}}},"cell_type":"code","id":"c62b73e4-e746-49f9-8106-921cbb4e6df8","execution_count":25,"outputs":[]},{"source":"# Set the query to be used for the QA system\nquery = \"How long is the circumference of the earth?\"\n\n# Invoke the query through the RetrievalQA instance and store the response\n\n\n# Print the response to the console\n","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":115,"type":"stream"},"1":{"height":77,"type":"stream"}}},"id":"f9f7a7f3-f0f1-44ad-be76-d15aa009ac34","cell_type":"code","execution_count":27,"outputs":[]},{"source":"## All done, congrats! ","metadata":{},"id":"65454beb-970f-4af6-a04c-798b9f665b6f","cell_type":"markdown"}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}